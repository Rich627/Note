Regression goal:$min||y-f(x, \theta)||, max C(y,f(x, \theta))$, C is correlation function
Multiple Logistic Regresson(MLR)
$min \ g(b)=min1/2||r(b)||^2$
$P(y) = \frac{1}{e^{-1(bo+b1x1+b2x2+...+bnxn)}}$
$logit=bo+b1x1+b2x2+...+bnxn$
$P=\frac{1}{1+e^{-logit}}(risk)$
$e^{logit}=\frac{P}{1-P}(odds)$
$logit =\frac{p}{1-p} = log(odds)$
left skewed 偏右
histogram可畫出data分佈or sns.distplot
Anderson-Darling test
用以檢測是否符合xx分佈, result<p-value就是那個分部
Jarque-Bera &Kolmogorov-Smirnov , Shapiro–Wilk, p-value>0.5 常態
Slightly right-skewed -> Square Root 
Moderately right-skewed -> Log 
Very right-skewed ->Reciprocal(倒數)
left-skewed -> Square
Counts ->Square Root
Proportions -> arcsin square root, $y=sin^{-1} \sqrt x$
rescaling $y=\frac{x-min(x)}{max(x)-min(x)}$
Robust$y=\frac{x-Q_1}{Q_3-Q_1}$
outlier:超過1.5IQR(四分位距)
L1  regularization:$||x||_1=\sum_{i=1}^n |x_i|$
L2  regularization:$||x||_2=\sqrt {x_1^2 +x_2^2+.x_n^2}$
用knn填NAN
Regression Loss:MSE, MAE, RMSE
Classifaction Loss:Binary$\cal L =\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)})]$, 
Categorical:$\cal L=\sum_c=1^C \sum_i=1^n -y_{c,i}log_2(p_{c,i})$
對角化(Decorrelated)
$f(y_1, y_2)=B^T[R]Y+\frac{1}{2}Y^T[A]Y$
ex.$f(x1, x2) = 6x_1^2-6x_1x_2+2x_2^2-x_1-2x_2$
$X =\begin{Bmatrix} x_1\\x_2 \end{Bmatrix}$
$B =\begin{Bmatrix} \frac{\partial f}{\partial x_1}\\ \cdots\\  \frac{\partial f}{\partial x_n}\end{Bmatrix}=\begin{Bmatrix} -1\\-2 \end{Bmatrix}$
$[A] =\begin{bmatrix}\frac{\partial^2 f}{\partial x_1^2}\ .. \ \frac{\partial^2 f}{\partial x_1 \partial x_n}\ \\ \frac {\partial^2 f}{\partial x_n \partial x_1}\ ..\ \frac {\partial^2 f}{\partial x_n^2}{}\end{bmatrix}=\begin{bmatrix} 12 & -6\\-6 & 4\end{bmatrix}$
To slove $[[A]−\lambda_i[I]]u_i =0$
$\begin{bmatrix}{12-\lambda_i} \ \  -6 \\ -6 \ \ \ \ \ \  {4-\lambda_i} \end{bmatrix}=\lambda_i^2-16\lambda_i+12=0$
$\lambda_i$ is called engenvalue
$u_i$ is eigenvector
Gradient Descent(梯度下降)
1. Initial values of x
2. Compute the search direction g from x, where g is called the search direction
3. Determine a step size α, $\alpha$ is hyperparmater.
4. Update the values of $x=x-\alpha g$
5. Repeat 1~4 until $\cal f$ converges
Steepest Descent 
1. Start with an arbitrary initial point $X_1$ . Set the iteration number as $i=1$
2.  Find the search direction $S_i$ as $S_i =−\nabla f_i =−\nabla f(X_i)$
3.  Determine the optimal step length $\lambda^*$in the direction $S_i$ and set
$X_{i+1} =X_i +\lambda^*_i S_i=X_i−\lambda^*_i\nabla f_i$
4.  Test the new point, $X_{i+1}$ , for optimality. If $X_{i+1}$ is optimum, stop the process. Otherwise go to step 5.
5.  Set the new iteration number $i=i+1$ and go to step 2.
terminate condition
1.函數值差不多
$|\frac{f(X_{i+1}-f(X_i))}{f(X_i)}| \leq \varepsilon$
2.梯度差不多
$|\frac{\partial f}{\partial x_i}|\leq \varepsilon$
3.x差不多
$|X_{i+1} - X_i| \leq \varepsilon$
Fletcher-Reeves Method(共軛梯度法)
每步和上一步垂直
1.Start with an arbitrary initial point $X1$.
2.Set the first search direction $S_1 = -\nabla f (X_1) = -\nabla f_1$
3.Find the point $X2$ according to the relation$X_2 = X_1 + \lambda^*_1 S_1$
4.Find $\nabla f = \nabla f(X_i)$,and set $S_i = -\nabla f_i +\frac{|\nabla f_i|^2}{|\nabla f_{i-1}|^2}S_{i-1}$
5Compute the optimum step length $\lambda_i^*$ in the direction $S_i$, and find the new point $X_{i+1} =X_i+\lambda_i^*S_i$
6.Test for the optimality of the point $X_{i+1}$. If $X_{i+1}$optimum, stop the process.
Otherwise set the value of $i=i+1$ and go to step 4.
Newton's Method
和梯度下降幾乎類似，唯一不同在於$X_{i+1}=X_i-[J_i]^{-1}\nabla f_i$, $[J_i]$ 是Hessian Matrix
Quasi-Newton Method
$X_{i+1}=X_i-[B_i]^{-1}\nabla f_i$,$[B_i]$ 是海森矩陣的反矩陣
$X_i+1 = X_i -\lambda_i^*[B_i]\nabla f(X_i)$
$S_i = -[B_i] \nabla f(X_i)$
BFGS
Levenberg-Marquardt Algorithm
SGD(Stochastic Gradient Descent)
Problem:
1.隨機性導致各方向增加速度不依所以會震盪
2.遇到local min, saddle points會卡住
3.使用minibatches導致導致迭代很慢
SGD+Momentum 
增加一個動量加速收斂加速收斂並減少波動性以及卡住的問題
更新x方式:$\cal v_{t+1}=\varrho \cal v_t +\nabla f(x_t)$, $x_{t+1}=x_t-\alpha \cal v_{t+1}$
AdaGrad 
RMSProp
Adam
rmsprop+momentum
Learning Rate $\eta$
step decay:half every few epochs
exponential decay:$\eta = \eta_0e^{-kt}$
1/t decay:$\eta =\eta /(1+kt)$
L-BFGS 在小型數據集上收斂得更快
Binary Cross-entropy
$J(w, b) =\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)})]$
Binary Classification Penalty:
cross-entropy
Hinge loss:
$\cal L(y, f(x)) = max(0, 1 - yf(x))$
sigmoid+cross-entropy
sigmoid+square
Softmax activation function
$s(y_i) = \frac {e^{x_i}}{\sum_{j=1}^{k} e^{x_j}}$
ex.
$p_{c, i}=\frac{e^{y_i}}{\sum_{j=1}^C e^{Y_j}}$
$\sum_{i=1}^n -y_{c, i}log(p_{c, i})$
$\cal L =-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{C}y_{ij}\log(p_{ij})$

| cat |3.2  | 1.3 | 2.2  |
| --- | --- | --- | ---- |
| car | 5.1  | 4.9 | 2.5  |
| frog | -1.7 | 2.0 | -3.1 |
col算$p(c,i)$
| cat  | 0.13  | 0.0252 | 0.4247 |
| ---- | ----- | ------ | ------ |
| car  | 0.86  | 0.9239 | 0.5732 |
| frog | 0.001 | 0.0508 | 0.0021 |
|      | 2.04     |   0.0791     |  6.1565  |
-ln(0.13)=2.04
-ln(0.0508)=0.0791
L=2.04+0.0791_6.1576
為什麼要正規化？
• 表達對權重的偏好
• 簡化模型，使其適用於測試數據
• 通過添加曲率改進優化
L2 regularization imposes a higher penalty on a larger value.
| L1                         | L2                                |
| -------------------------- | --------------------------------- |
| sparse solution            | noe-sparse                        |
| multiple sol.              | one sol.                          |
| built-in feature selection | no feacture selection             |
| 會受異常值影響 | 不受異常值影響(due to square) |
  
Batch Normalization進如神經元前做mean=0, variance=1分佈轉換
減少了訓練時間，並使非常深的網絡可訓練。  由於較少的協變量偏移，可以使用較大的學習率。 
減少梯度消失/爆炸問題 
學習受初始化的影響較小。 
BN 減少了對正則化的需求。
